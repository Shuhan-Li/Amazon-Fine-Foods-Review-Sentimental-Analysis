{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "seed0=1337\n",
    "np.random.seed(seed0)\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "tf.random.set_seed(seed0)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split large data file to smaller copy\n",
    "\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# extract every category for 10000\n",
    "df_1 = df[df['Score'] == 1].sample(n=10000, random_state=1)\n",
    "df_2 = df[df['Score'] == 2].sample(n=10000, random_state=1)\n",
    "df_3 = df[df['Score'] == 3].sample(n=20000, random_state=1)\n",
    "df_4 = df[df['Score'] == 4].sample(n=10000, random_state=1)\n",
    "df_5 = df[df['Score'] == 5].sample(n=10000, random_state=1)\n",
    "\n",
    "# 将所有抽取的部分合并成一个新的DataFrame\n",
    "df_extract = pd.concat([df_1, df_2, df_3, df_4, df_5])\n",
    "\n",
    "# 将抽取的数据保存到新的CSV文件\n",
    "df_extract.to_csv('Reviews_10000.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reviews_10000.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_processed = df[['Text','Score']]\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_processed[df_processed['Score']==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Help funciton to classify based on score\n",
    "def score_converter(score):\n",
    "    if score <= 2:\n",
    "        return 'unsatisfied'\n",
    "    elif score>=4:\n",
    "        return 'satisfied'\n",
    "\n",
    "# Helper function to clean the text\n",
    "def remove_tags(string):\n",
    "    result =re.sub(r'<br\\s*/?>', '', string)\n",
    "    result = re.sub('https://.*','',result)   #remove URLs\n",
    "    result = re.sub('[^a-zA-Z0-9 ]', '', result)    #remove non-alphanumeric characters\n",
    "    result = result.lower()\n",
    "    return result\n",
    "\n",
    "df_processed['Category'] = df_processed['Score'].apply(score_converter)\n",
    "df_processed['Text'] = df_processed['Text'].apply(remove_tags)\n",
    "\n",
    "df_processed = df_processed.sample(frac = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove the stop word to increase model efficiency\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_processed['Text'] = df_processed['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to WordNet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Get part-of-speech tags for each word\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        # Convert part-of-speech tag to a format recognized by WordNetLemmatizer\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:\n",
    "            # If the tag is not recognized, keep the word as is\n",
    "            lemmatized_words.append(word)\n",
    "        else:\n",
    "            # Lemmatize the word with the appropriate part of speech tag\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos=wntag))\n",
    "\n",
    "    # Return the lemmatized words as a single string\n",
    "    return ' '.join(lemmatized_words)\n",
    "df_processed['Text'] = df_processed['Text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df_processed = pd.read_csv(\"processed.csv\")\n",
    "df_processed  = df_processed.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_processed[\"Category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = 0.0\n",
    "for i in df_processed['Text']:\n",
    "    word_list = i.split()\n",
    "    s = s + len(word_list)\n",
    "print(\"Average length of each review : \",s/df_processed.shape[0])\n",
    "pos = 0\n",
    "neg = 0\n",
    "for i in range(df_processed.shape[0]):\n",
    "\n",
    "    if df_processed.iloc[i]['Category'] == 'satisfied':\n",
    "        pos = pos + 1\n",
    "    elif df_processed.iloc[i]['Category'] == 'unsatisfied':\n",
    "        neg+=1\n",
    "print(\"Percentage of reviews with positive sentiment is \"+str(pos/df_processed.shape[0]*100)+\"%\")\n",
    "print(\"Percentage of reviews with negative sentiment is \"+str(neg/df_processed.shape[0]*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reviews = df_processed['Text'].values\n",
    "labels = df_processed['Category'].values\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify = encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_acc(his,title):\n",
    "    plt.plot(his.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(his.history['val_accuracy'], label='Validation Accuracy')\n",
    "    # add label and tile\n",
    "    plt.title(title+' Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 6000\n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 200\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Construct simple NN network\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(3000, 100, input_length=max_length))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_padded, train_labels,\n",
    "                    epochs=15, verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "class_predictions = np.where(prediction > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, class_predictions)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "plot_acc(history,\"Simple NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_cnn = tf.keras.Sequential()\n",
    "model_cnn.add(tf.keras.layers.Embedding(input_dim=3000, output_dim=100, input_length=max_length))\n",
    "model_cnn.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model_cnn.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "model_cnn.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model_cnn.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_cnn.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_padded, train_labels,\n",
    "                    epochs=15, verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction = model_cnn.predict(test_padded)\n",
    "class_predictions = np.where(prediction > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, class_predictions)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "plot_acc(history,\"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#attention layer\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    Attention(return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.03,momentum=0.8) ,\n",
    "              metrics=['accuracy'])\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_padded, train_labels,\n",
    "                    epochs=70, verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test_padded)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_predictions = np.where(prediction > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, class_predictions)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "plot_acc(history, \"Bi-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_out=0.2\n",
    "activation=LeakyReLU(alpha = 0.01)\n",
    "regularizer=regularizers.l2(2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_cb = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=8,\n",
    "                          strides=1,\n",
    "                          activation=activation,\n",
    "                          padding='causal'),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    Attention(return_sequences=False),\n",
    "    tf.keras.layers.Dense(32, activation=activation, kernel_regularizer = regularizer),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# compile model\n",
    "model_cb.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "# model summary\n",
    "model_cb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model_cb.fit(train_padded, train_labels,\n",
    "                    epochs=2, verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model_cb.predict(test_padded)\n",
    "class_predictions = np.where(prediction > 0.5, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, class_predictions)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_acc(history,\"CNN-LSTM Combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}